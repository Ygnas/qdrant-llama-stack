---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
data:
  run.yaml: |
    version: "2"
    image_name: rh
    apis:
    - agents
    - datasetio
    - files
    - inference
    - safety
    - scoring
    - tool_runtime
    - vector_io
    providers:
      inference:
      - provider_id: sentence-transformers
        provider_type: inline::sentence-transformers
        config: {}
      - provider_id: ${env.VLLM_URL:+vllm-inference-1}
        provider_type: remote::vllm
        config:
          base_url: ${env.VLLM_URL:=}
          max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
          api_token: ${env.VLLM_API_TOKEN:=fake}
          tls_verify: ${env.VLLM_TLS_VERIFY:=true}
      vector_io:
      - provider_id: ${env.ENABLE_QDRANT:+qdrant}
        provider_type: remote::qdrant
        config:
          location: ${env.QDRANT_LOCATION:=}
          url: ${env.QDRANT_URL:=}
          port: ${env.QDRANT_PORT:=6333}
          grpc_port: ${env.QDRANT_GRPC_PORT:=6334}
          prefer_grpc: ${env.QDRANT_PREFER_GRPC:=false}
          https: ${env.QDRANT_HTTPS:=false}
          api_key: ${env.QDRANT_API_KEY:=}
          prefix: ${env.QDRANT_PREFIX:=}
          timeout: ${env.QDRANT_TIMEOUT:=}
          host: ${env.QDRANT_HOST:=}
          persistence:
            namespace: vector_io::qdrant_remote
            backend: kv_default
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence:
            agent_state:
              backend: kv_default
              namespace: agents
            responses:
              backend: sql_default
              table_name: responses
      eval: []
      files:
      - provider_id: meta-reference-files
        provider_type: inline::localfs
        config:
          metadata_store:
            backend: sql_default
            table_name: files_metadata
          storage_dir: /opt/app-root/src/.llama/distributions/rh/files
      datasetio:
      - provider_id: huggingface
        provider_type: remote::huggingface
        config:
          kvstore:
            backend: kv_default
            namespace: datasetio::huggingface
      scoring:
      - provider_id: basic
        provider_type: inline::basic
        config: {}
      - provider_id: llm-as-judge
        provider_type: inline::llm-as-judge
        config: {}
      tool_runtime:
      - provider_id: rag-runtime
        provider_type: inline::rag-runtime
        config: {}
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
    metadata_store:
      type: sqlite
      db_path: /opt/app-root/src/.llama/distributions/rh/inference_store.db
    storage:
      backends:
        kv_default:
          db_path: /opt/app-root/src/.llama/distributions/rh/kvstore.db
          type: kv_sqlite
        sql_default:
          db_path: /opt/app-root/src/.llama/distributions/rh/sql_store.db
          type: sql_sqlite
      stores:
        conversations:
          backend: sql_default
          table_name: openai_conversations
        inference:
          backend: sql_default
          table_name: inference_store
        metadata:
          backend: kv_default
          namespace: registry
    registered_resources:
      models:
      - provider_id: sentence-transformers
        model_id: sentence-transformers/ibm-granite/granite-embedding-125m-english
        provider_model_id: ibm-granite/granite-embedding-125m-english
        model_type: embedding
        metadata:
          embedding_dimension: 768
      - provider_id: vllm-inference-1
        model_id: ${env.INFERENCE_MODEL:=llama-3-2-3b-instruct}
        model_type: llm
        metadata:
          display_name: ${env.INFERENCE_MODEL:=llama-3-2-3b-instruct}
      shields: []
      vector_dbs: []
      datasets: []
      scoring_fns: []
      benchmarks: []
      tool_groups:
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
    server:
      port: 8321
    vector_stores:
      default_provider_id: qdrant
      default_embedding_model:
        provider_id: sentence-transformers
        model_id: ibm-granite/granite-embedding-125m-english
---
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llama-stack-distribution-qdrant
spec:
  replicas: 1
  server:
    containerSpec:
      resources:
        requests:
          cpu: "250m"
          memory: "500Mi"
        limits:
          cpu: "8"
          memory: "12Gi"
      env:
        - name: INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              key: INFERENCE_MODEL
              name: llama-stack-inference-model-secret
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              key: VLLM_URL
              name: llama-stack-inference-model-secret
        - name: VLLM_TLS_VERIFY
          valueFrom:
            secretKeyRef:
              key: VLLM_TLS_VERIFY
              name: llama-stack-inference-model-secret
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              key: VLLM_API_TOKEN
              name: llama-stack-inference-model-secret
        - name: ENABLE_QDRANT
          value: "true"
        - name: QDRANT_HOST
          value: qdrant
        - name: QDRANT_PORT
          value: "6333"
        - name: QDRANT_API_KEY
          valueFrom:
            secretKeyRef:
              name: qdrant-secret
              key: api-key
      command:
        - /bin/sh
        - -c
        - |
          exec llama stack run /etc/llama-stack/run.yaml
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
    userConfig:
      configMapName: llama-stack-config
